# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZQ_O8yDntuOig9fkey4NVMEvK4fFMAXm
"""

# app.py
import streamlit as st
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from io import StringIO

st.set_page_config(page_title="LIAR Fake-Claim Classifier", layout="centered")

st.title("LIAR — Fake Claim Classifier (BERT)")
st.markdown(
    """
Enter a political statement to predict whether it's likely *fake* or *true*, or upload a CSV with a `statement` column for batch predictions.

**Notes**
- Model directory must contain both model files and tokenizer (like a folder saved with `trainer.save_model(...)` and `tokenizer.save_pretrained(...)`).
- For speed, run this app on Colab/VM/GPU if available, otherwise CPU will be slower.
"""
)

# --- Sidebar: Model path / options ---
st.sidebar.header("Model & Settings")
model_dir = st.sidebar.text_input("/content/drive/MyDrive/LIAR/model", value="/content/drive/MyDrive/LIAR/model")
max_length = st.sidebar.number_input("Max token length", min_value=32, max_value=512, value=128)
device_option = st.sidebar.selectbox("Device", options=["cpu", "cuda"] if torch.cuda.is_available() else ["cpu"])
device = torch.device(device_option)

load_button = st.sidebar.button("Load model")

@st.cache_resource
def load_model_tokenizer(model_dir):
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)
    model.to(device)
    model.eval()
    return tokenizer, model

tokenizer = None
model = None
if load_button:
    try:
        with st.spinner("Loading model..."):
            tokenizer, model = load_model_tokenizer(model_dir)
        st.sidebar.success("Model loaded ✅")
    except Exception as e:
        st.sidebar.error(f"Failed to load model: {e}")

# If model not loaded yet, try to load once (helpful for quick start)
if tokenizer is None or model is None:
    try:
        tokenizer, model = load_model_tokenizer(model_dir)
        st.sidebar.success("Model auto-loaded")
    except Exception:
        pass

# Helper: get label names
def get_label_names(model):
    num_labels = getattr(model.config, "num_labels", None)
    # If 2 labels, assume binary mapping: 0 -> fake, 1 -> true
    if num_labels == 2:
        return {0: "fake", 1: "true"}
    # If >2, return numeric label names (user can adapt)
    return {i: f"label_{i}" for i in range(num_labels)}

def predict_texts(model, tokenizer, texts, max_length=128):
    enc = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    )
    enc = {k: v.to(device) for k, v in enc.items()}
    with torch.no_grad():
        outputs = model(**enc)
        logits = outputs.logits
        # softmax for multi-class probabilities
        probs = torch.softmax(logits, dim=1).cpu().numpy()
        preds = probs.argmax(axis=1)
    return preds, probs

# UI: Single prediction
st.header("Single statement prediction")
statement = st.text_area("Enter statement", height=120, value="The economy grew by 5% last year.")
if st.button("Predict (single)"):
    if model is None or tokenizer is None:
        st.error("Model not loaded. Enter correct model directory in the sidebar and click 'Load model'.")
    elif not statement.strip():
        st.error("Please enter a statement to classify.")
    else:
        with st.spinner("Predicting..."):
            preds, probs = predict_texts(model, tokenizer, [statement], max_length=max_length)
            label_names = get_label_names(model)
            pred_label = label_names[int(preds[0])]
            pred_prob = float(probs[0, int(preds[0])])

        st.subheader("Result")
        st.write(f"**Predicted label:** {pred_label}")
        st.write(f"**Probability:** {pred_prob:.4f}")
        # show full prob distribution
        df_probs = pd.DataFrame(probs, columns=[label_names[i] for i in range(probs.shape[1])])
        st.write("Probability distribution:")
        st.dataframe(df_probs.T.rename(columns={0: "probability"}))

# UI: Batch prediction via CSV
st.header("Batch prediction (CSV upload)")
st.markdown("Upload a CSV file with a `statement` column. The app will add `pred_label` and `pred_probability` columns and let you download results.")

uploaded_file = st.file_uploader("Upload CSV", type=["csv"])
if uploaded_file is not None:
    try:
        raw = uploaded_file.read().decode("utf-8")
        df_in = pd.read_csv(StringIO(raw))
    except Exception as e:
        st.error(f"Failed to read CSV: {e}")
        df_in = None

    if df_in is not None:
        if "statement" not in df_in.columns:
            st.error("CSV must contain a column named 'statement'.")
        else:
            st.write("Preview:")
            st.dataframe(df_in.head())

            if st.button("Run batch prediction"):
                if model is None or tokenizer is None:
                    st.error("Model not loaded. Enter correct model directory in the sidebar and click 'Load model'.")
                else:
                    with st.spinner("Predicting batch..."):
                        texts = df_in["statement"].astype(str).tolist()
                        preds, probs = predict_texts(model, tokenizer, texts, max_length=max_length)
                        label_names = get_label_names(model)
                        pred_labels = [label_names[int(p)] for p in preds]
                        pred_probs = [float(probs[i, int(preds[i])]) for i in range(len(preds))]

                        df_out = df_in.copy()
                        df_out["pred_label"] = pred_labels
                        df_out["pred_probability"] = pred_probs

                        st.success("Batch prediction done.")
                        st.dataframe(df_out.head())

                        # Save to CSV and provide download link
                        csv_buffer = df_out.to_csv(index=False).encode("utf-8")
                        st.download_button("Download predictions CSV", data=csv_buffer, file_name="liar_predictions.csv", mime="text/csv")

# Footer / tips
st.markdown("---")
st.write("Tips:")
st.write("- If your model was trained with different label encodings, update `get_label_names()` accordingly.")
st.write("- For faster performance use a GPU runtime or smaller models (DistilBERT).")

pip install streamlit transformers torch

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit



# Commented out IPython magic to ensure Python compatibility.
# %pip install pyngrok